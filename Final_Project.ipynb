{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "exact-extension",
   "metadata": {},
   "source": [
    "# Exoplanet\n",
    "<!-- Author: Xiaorong Yan -->\n",
    "<!-- Date:  2021/12/15-->\n",
    "\n",
    "The goal of this project is to use skills I learned from data science class to solve a problem I find interesting in, in this case, astronomy related. Since I major in CS and minor in astronomy, this will be my first ever project for me to combine my knowledge in both field. The dataset I will be using is from NASA's exoplanet archive. Detail see below. \n",
    "Found in https://github.com/awesomedata/awesome-public-datasets.\n",
    "\n",
    "Exoplanet data available from \n",
    "https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=PS&constraint=default_flag=1\n",
    "and\n",
    "https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI\n",
    "NASA Exoplanet Archive\n",
    "To use up-to-date data, you can either: \n",
    "a) download from the link\n",
    "b) use NASA's API (refer to https://exoplanetarchive.ipac.caltech.edu/docs/program_interfaces.html)\n",
    "\n",
    "From NASA Exoplanet Archive's front page, we can see that there are total of 4877 confirmed exoplanet, 173 of which came from TESS[1], which has 4708 candidates (as of December 2021). If we can confirm or disqualify potential candidate from TESS, we will have more exoplanets to study, maybe even confirm more Earth like exoplanets. From the data science class, I recalled that we used sklearn to train models on dataset to predict results. Further examing the dataset confirmed my idea that using what I have learned to predict condidates is indeed doable. So here we go. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1] Transit Surveys. Launched in April 2018, TESS is surveying the sky for two years to find transiting exoplanets around the brightest stars near Earth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "israeli-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import bs4\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# displaying all data in a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-watershed",
   "metadata": {},
   "source": [
    "The downloaded csv files are not well formatted since the top 1*n cells are name description, so pd.read_csv() does not work nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reading in dataset using pandas\n",
    "df = pd.read_csv('TOI_202112.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
